# What is information?

Adami
2016
created: 8-8-2017

## synopsis

Entropy quantifies what you don't know.
Entropy is "in the eye of the beholder."
Discussing the innate entropy of a physical system is senseless.
In order to discuss the entropy of a physical system, you must define a discrete measure on the system that can take on a finite number of values.
This basically turns observations made on your physical system into something like observations on a mathematical random variable.
In order to have a sensible discussion of entropy of a mathematical random variable, you at least need to know how many states it can take on.
This is taken as a given in information theory.

Entropy is described using logarithms in order for the addition operation between the entropy of two independent systems to make sense.
If one system has *m* possible states and another has *n* possible states, then together there are *m \times n* possible states.
The useful property is *\log(m \times n) = \log(m) + \log(n)*.

Information is the difference between two entropies.

Maximum entropy is achieved when a random variable has a uniform probability distribution.
Adami suggests that it is useful to think of all non-uniform discrete probability distributions as conditional.
What it's conditional on is something like you knowing the probability distribution, which gives you information (a reduction in entropy) right off the bat.

Adami identifies two ways to reduce entropy when working with physical systems:
  1. repeated statistical measurement, and
  2. theory.

Both allow the viewer to (perhaps) hypothesize a non-uniform probability distribution.
In physical systems, the true probability distribution cannot be known with absolute certainty.
Thus, it makes sense to talk of a reduction in the *expected* entropy in these cases.
In physical systems, any non-maximal entropy (i.e. the entropy of a system with non-uniform probability distribution between its states) must be an expected entropy.

## misc
"You may not even know the range of *i*.
If that's the case, we are really up the creek, with paddle *in abstentia*."
